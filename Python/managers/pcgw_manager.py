import logging
import time
import requests
try:
    import cloudscraper
except ImportError:
    cloudscraper = None
from urllib.parse import quote, unquote, urlparse
from bs4 import BeautifulSoup

class PCGWManager:
    """
    Manages interactions with the PCGamingWiki API.
    Adheres to MediaWiki API etiquette regarding User-Agent and rate limiting.
    """
    
    def __init__(self):
        # Use a standard browser User-Agent to avoid WAF/Cloudflare blocks (403 Forbidden)
        # While PCGW asks for custom UAs, their WAF often blocks non-browser UAs.
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        }

        if cloudscraper:
            # Use cloudscraper to bypass Cloudflare/WAF protection
            self.session = cloudscraper.create_scraper()
            # Update headers but preserve the User-Agent generated by cloudscraper to ensure bypass works
            self.session.headers.update({k: v for k, v in self.headers.items() if k != 'User-Agent'})
        else:
            self.session = requests.Session()
            self.session.headers.update(self.headers)

    def fetch_data(self, game_name, steam_id=None):
        """
        Fetches metadata from PCGamingWiki.
        
        Args:
            game_name (str): The name of the game to search for.
            steam_id (str, optional): The Steam AppID to resolve the page.
            
        Returns:
            dict: A dictionary containing parsed PCGW data, or None if failed.
        """
        pcgw_data = None
        page_name = None

        # 1. Try to resolve page name via AppID redirect
        if steam_id and str(steam_id) not in ['NOT_FOUND_IN_DATA', 'ITEM_IS_NONE']:
            appid_url = f"https://www.pcgamingwiki.com/api/appid.php?appid={steam_id}"
            try:
                # Requests follows redirects by default. We check the final URL.
                response = self.session.get(appid_url, timeout=10)
                if response.status_code == 200 and "/wiki/" in response.url:
                    # Extract title from URL (e.g. .../wiki/Game_Title)
                    path = urlparse(response.url).path
                    if "/wiki/" in path:
                        page_name = unquote(path.split("/wiki/")[-1])
                        # Normalize underscores to spaces for consistency
                        page_name = page_name.replace('_', ' ')
                        logging.info(f"Resolved PCGW page via AppID {steam_id}: {page_name}")
            except Exception as e:
                logging.warning(f"PCGW AppID lookup failed: {e}")

        if not page_name:
            # Fallback to provided game name
            page_name = game_name

        # 2. Use MediaWiki Parse API to get structured HTML
        api_url = "https://www.pcgamingwiki.com/api.php"
        params = {
            'action': 'parse',
            'page': page_name,
            'format': 'json',
            'redirects': 1  # Automatically resolve redirects
        }

        try:
            time.sleep(0.5) # Polite delay
            response = self.session.get(api_url, params=params, timeout=10)
            
            soup = None
            if response.status_code == 200:
                data = response.json()
                
                if 'error' in data:
                    logging.warning(f"PCGW API returned error for '{page_name}': {data['error'].get('info')}")
                elif 'parse' in data and 'text' in data['parse']:
                    html_content = data['parse']['text']['*']
                    soup = BeautifulSoup(html_content, 'html.parser')
                    # Update page_name to the actual resolved title from the API
                    if 'title' in data['parse']:
                        page_name = data['parse']['title']
            elif response.status_code == 403:
                logging.error(f"PCGW API 403 Forbidden. Check User-Agent headers.")
            
            # Fallback to scraping only if API failed but we suspect the page exists
            if not soup and response.status_code != 404:
                wiki_url = f"https://www.pcgamingwiki.com/wiki/{quote(page_name.replace(' ', '_'))}"
                soup = self._scrape_html(wiki_url)

            if soup:
                pcgw_data = self._parse_soup(soup)

        except Exception as e:
            logging.error(f"Failed to fetch PCGW data for {game_name}: {e}")

        return pcgw_data

    def _scrape_html(self, url):
        """Fallback HTML scraping using compliant headers."""
        try:
            time.sleep(1)
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            return BeautifulSoup(response.content, 'html.parser')
        except Exception as e:
            logging.error(f"PCGW HTML scrape failed for {url}: {e}")
            return None

    def _parse_soup(self, soup):
        """Parses the BeautifulSoup object to extract relevant data."""
        pcgw_data = {}
        
        # Parse Infobox
        infobox = soup.find('table', class_='infobox')
        if infobox:
            rows = infobox.find_all('tr')
            for row in rows:
                th = row.find('th')
                td = row.find('td')
                if th and td:
                    key = th.get_text(strip=True)
                    value = td.get_text(strip=True)
                    
                    if key in ['Developers', 'Engines', 'Release dates', 'Reception', 'Taxonomy', 'General information']:
                        pcgw_data[key] = value
                    elif 'DLC' in key or 'Expansion' in key:
                        if 'dlc' not in pcgw_data:
                            pcgw_data['dlc'] = []
                        pcgw_data['dlc'].append(value)
                    elif 'Notes' in value or 'disabled' in value.lower():
                        if 'features_with_notes' not in pcgw_data:
                            pcgw_data['features_with_notes'] = []
                        pcgw_data['features_with_notes'].append({key: value})

        # Parse Save and Config Locations
        save_locations = {}
        config_locations = {}
        
        content = soup.find('div', class_='mw-parser-output')
        if content:
            headers = content.find_all(['h2', 'h3', 'h4'])
            for header in headers:
                header_text = header.get_text(strip=True)
                if 'Save game data location' in header_text:
                    next_elem = header.find_next(['ul', 'table'])
                    if next_elem:
                        self._parse_locations(next_elem, save_locations)
                elif 'Configuration file' in header_text or 'Config file' in header_text:
                    next_elem = header.find_next(['ul', 'table'])
                    if next_elem:
                        self._parse_locations(next_elem, config_locations)
        
        pcgw_data['save_locations'] = save_locations
        pcgw_data['config_locations'] = config_locations
        return pcgw_data

    def _parse_locations(self, element, locations_dict):
        """Helper to parse location lists or tables."""
        if element.name == 'ul':
            lis = element.find_all('li')
            for li in lis:
                text = li.get_text(strip=True)
                if ':' in text:
                    platform, path = text.split(':', 1)
                    platform = platform.strip()
                    path = path.strip()
                    if platform not in locations_dict:
                        locations_dict[platform] = []
                    locations_dict[platform].append(path)
        elif element.name == 'table':
            rows = element.find_all('tr')
            for row in rows:
                cells = row.find_all('td')
                if len(cells) >= 2:
                    platform = cells[0].get_text(strip=True)
                    path = cells[1].get_text(strip=True)
                    if platform not in locations_dict:
                        locations_dict[platform] = []
                    locations_dict[platform].append(path)
